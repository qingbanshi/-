# 6.1 参数的更新

神经网络的目的:找到使损失函数值最小的参数---->**最优化**

## 6.1.2随机梯度下降法__SGD

![image-20200702160055596](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200702160055596.png)

## 6.1.4 Momentum法

![image-20200702160127972](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200702160127972.png)

具体操作:将alpha视为阻力 v视作速度 更新时第一个v是0 alpha默认为0.9即:

![image-20200702172114087](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200702172114087.png)

![image-20200702172122158](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200702172122158.png)

由于v具有一定程度上的滞后性(相较于直接的梯度),故而在某些反向上的抖动会小很多:

图一:随机梯度下降

![image-20200702172444403](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200702172444403.png)

图二:动量法

![image-20200702172503140](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200702172503140.png)

## 6.1.5AdaGrad(学习率衰减)

![image-20200702205104468](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200702205104468.png)

拉姆达--学习率  h--过去所有梯度的平方和

此次基于乘以h^-(1/2)^进行学习率的更新 能够使得学习率渐缓 影响较大的节点反而变换得快

其效果如图

![image-20200702213304742](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200702213304742.png)

### **一些缺点**:

到后期会遇上学习率趋于零的尴尬境地(h记录了过去所有的参数,最终会变得很大)

### **解决方案**:RMSPorp:

会将过去的参数渐渐"遗忘",从而加大新梯度的影响能力 被称为"指数移动平均"---呈指数函数式地减小过去梯度的尺度

## 6.1.6Adam

此方法将以上两种方法融合: 可变学习率的动量法(注意:并不完全正确)

还将进行参数空间的偏置矫正

相关超参数:alpha学习率  beta1/beta2两个momentum参数,一般令beta1 = 0.9 ;beta2 =0.999 

# 6.2 权重的初始值

## 6.2.1初始值不能对称

关于一种方法:权值衰减---通过减少权值来抑制过拟合的发生

为了防止更新时的一致性---不使用相同的权重(瓦解权重的对称结构)(乘法节点反向传播时将权重交换)

## 6.2.2隐藏层的激活值的分布(以sifmiod函数为例)

### "梯度消失"

使用标准差为一的高斯分布时,各层激活函数输出值偏向一:

![image-20200703140619278](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200703140619278.png)

因为sigmiod![image-20200703140728868](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200703140728868.png)函数在0与1上导数趋于零,故偏向0,1的数据分布会使反向传播中的梯度变小,最后消失.从而失去反向传播的可能性 被称之为梯度消失

### "表现力受限"

当初始权重值标准差为.01时,分布则集中在0.5附近,,不会有此问题:

![image-20200703142653668](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200703142653668.png)

但是 如果神经节都集中在同一个位置 则这些神经节都是无意义的,因为可以用一个神经节来达成同样的效果,这种问题被称之为"表现力受限"

### Xavier权重值

为了解决上述问题,提出了一种叫做Xavier的权重初始值:

#### 分布情况:

如果前一层节点数为n,则此层使用标准差为n^-1/2^的高斯分布,则会呈现出较为多元的分布

![image-20200703144215607](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200703144215607.png)

如果想要解决歪斜问题,则将激活函数改为tanh函数即可(应为sigmiod函数关于(0,0.5)对称)

## 6.2.3 ReLu的初始值

Xavier权重值是为线性函数设计出的初始值,对于ReLu函数则显得有些不适应

ReLu![image-20200703144638525](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200703144638525.png)函数

### He初始值

当前一层节点数为n时,使用标准差为sqrt(2/n)的高斯分布

![image-20200703144957572](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200703144957572.png)

# 6.3 BatchNormalization(分支正规化)

为了使各层的激活值拥有更多广度,"强制性"地调整每层的激活值

优点:

1. 学习率可以适当上调(学习速度up)
2. 不必过于依赖初始值
3. 抑制了过拟合

## 6.3.1算法

具体的方法是在各层之间插入正规化层以完成正规化

![image-20200703153344018](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200703153344018.png)

原理:通过计算均值来实现每层数据"方差为1,均值为零"的正规化

![image-20200703154010091](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200703154010091.png)先求均值,再求方差

# 6.4 正则化

## 6.4.1过拟合

发生原因:

* 模型有大量参数 表现力强

* 训练数据不足

  ## 6.4.2 权值衰减

  通过在学习过程中对过大权重进行惩罚(使其减少)来抑制过拟合

  ### 衰减方法

  通过给损失函数的值加上范数来实现反向传播是权重的减小

  ### 常用范数

  #### L2范数: 

  最常用的范数![image-20200704093627432](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200704093627432.png)

  其权值衰减为![image-20200704093654009](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200704093654009.png)(拉姆达为超参数,需要调整) 其中,1/2是为了将平方项系数去掉(求导后)

  ![image-20200704103923896](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200704103923896.png)(在每一层的dW处)

  ## 6.4.3DropOut

  随机选出并删除隐藏层神经元

  ![image-20200704104239424](E:\Programing Learning\Python\Deep Learning Learning\学习笔记\06 一些技巧\图片\image-20200704104239424.png)

  

  

机器学习常用多个相同网络进行学习,再集成输出从而提高百分点 使用Dropout可以将结构改变

# 6.5 超参数的验证

## 6.5.1验证数据

为防止超参数对训练数据过拟合,特地划分出**验证数据**以进行超参数的矫正

## 6.5.2 超参数的最优化

为防止训练数据按序排列,在抽取验证数据时要将训练数据打乱抽取

### 具体方法

1. 设定超参数的范围
2. 从设定的超参数范围中随机采样
3. 使用采样到的超参数进行学习,并用验证数据评估精度
4. 重复步骤2与3(100次等)根据识别精度结果,调整超参数范围

### 注意事项

* 超参数采样时随机搜索优于有序搜索
* 超参数的大致范围是对数尺度上的大致(指定位数)
* 要尽量减少学习的epoch(),从而缩短一次评估所需时间
* 尽早放弃不合逻辑的超参数

## 6.5.3 超参数最优化的实现